{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises \n",
    "# 1. Tune the network\n",
    "Run the experiment below, explore the different parameters (see suggestions below) and study the result with tensorboard. \n",
    "Make a single page (1 a4) report of your findings. Use your visualisation skills to communicate your most important findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "from mltrainer import imagemodels, Trainer, TrainerSettings, ReportTypes, metrics\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from tomlserializer import TOMLSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torch cuda version:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"cuda device count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"using cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using `tomlserializer` to easily keep track of our experiments, and to easily save the different things we did during our experiments.\n",
    "It can export things like settings and models to a simple `toml` file, which can be easily shared, checked and modified.\n",
    "\n",
    "First, we need the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "preprocessor = BasePreprocessor()\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=64, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to determine how well our model is performing. We will use accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set up a single experiment.\n",
    "\n",
    "- We will show the model batches of 64 images, \n",
    "- and for every epoch we will show the model 100 batches (trainsteps=100).\n",
    "- then, we will test how well the model is doing on unseen data (teststeps=100).\n",
    "- we will report our results during training to tensorboard, and report all configuration to a toml file.\n",
    "- we will log the results into a directory called \"modellogs\", but you could change this to whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=3,\n",
    "    metrics=[accuracy],\n",
    "    logdir=\"modellogs\",\n",
    "    train_steps=100,\n",
    "    valid_steps=100,\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a very basic model: a model with three linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes: int, units1: int, units2: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.units1 = units1\n",
    "        self.units2 = units2\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units1, units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    num_classes=10, units1=256, units2=256)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I developped the `tomlserializer` package, it is a useful tool to save configs, models and settings as a tomlfile; that way it is easy to track what you changed during your experiments.\n",
    "\n",
    "This package will 1. check if there is a `__dict__` attribute available, and if so, it will use that to extract the parameters that do not start with an underscore, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in model.__dict__.items() if not k.startswith(\"_\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that if you want to add more parameters to the `.toml` file, eg `units3`, you can add them to the class like this:\n",
    "\n",
    "```python\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes: int, units1: int, units2: int, units3: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.units1 = units1\n",
    "        self.units2 = units2\n",
    "        self.units3 = units3  # <-- add this line\n",
    "```\n",
    "\n",
    "And then it will be added to the `.toml` file. Check the result for yourself by using the `.save()` method of the `TomlSerializer` class like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomlserializer = TOMLSerializer()\n",
    "tomlserializer.save(settings, \"settings.toml\")\n",
    "tomlserializer.save(model, \"model.toml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `settings.toml` and `model.toml` files to see what is in there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script for looping through some epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "units = [64, 32, 16]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "main_folder = \"modellogs\"\n",
    "subfolder = \"change_epochs\"\n",
    "amount_of_epochs = [5, 8, 10]\n",
    "\n",
    "for epochs in amount_of_epochs:\n",
    "    epoch_subfolder = f\"{subfolder}/epochs_{epochs}\"\n",
    "    settings = TrainerSettings(\n",
    "        epochs=epochs,\n",
    "        metrics=[accuracy],\n",
    "        logdir=f\"{main_folder}/{epoch_subfolder}\",\n",
    "        train_steps=len(train),\n",
    "        valid_steps=len(valid),\n",
    "        reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    "    )\n",
    "\n",
    "    for unit1 in units:\n",
    "        for unit2 in units:\n",
    "            if unit2 <= unit1:\n",
    "                print(f\"Epochs: {epochs}, Units: {unit1}, {unit2}\")\n",
    "                model = NeuralNetwork(num_classes=10, units1=unit1, units2=unit2)\n",
    "\n",
    "                trainer = Trainer(\n",
    "                    model=model,\n",
    "                    settings=settings,\n",
    "                    loss_fn=loss_fn,\n",
    "                    optimizer=optim.Adam,\n",
    "                    traindataloader=trainstreamer,\n",
    "                    validdataloader=validstreamer,\n",
    "                    scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                )\n",
    "                trainer.loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script for changing units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "units = [2**i for i in range(4, 11)]  # 16, 32, 64, ..., 1024\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "main_folder = \"modellogs\"\n",
    "subfolder = \"change_units\"\n",
    "\n",
    "for unit1 in units:\n",
    "    for unit2 in units:\n",
    "        if unit2 <= unit1:\n",
    "            run_subfolder = f\"{subfolder}/units_{unit1}_{unit2}\"\n",
    "            settings = TrainerSettings(\n",
    "                epochs=3,\n",
    "                metrics=[accuracy],\n",
    "                logdir=f\"{main_folder}/{run_subfolder}\",\n",
    "                train_steps=len(train),\n",
    "                valid_steps=len(valid),\n",
    "                reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    "            )\n",
    "            print(f\"Units: {unit1}, {unit2}\")\n",
    "            model = NeuralNetwork(num_classes=10, units1=unit1, units2=unit2)\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                settings=settings,\n",
    "                loss_fn=loss_fn,\n",
    "                optimizer=optim.Adam,\n",
    "                traindataloader=trainstreamer,\n",
    "                validdataloader=validstreamer,\n",
    "                scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            )\n",
    "            trainer.loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script for changing batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batchsizes = [4,32,128]\n",
    "units = [64, 32, 16]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "main_folder = \"modellogs\"\n",
    "subfolder = \"change_batchsize\"\n",
    "\n",
    "for batchsize in batchsizes:\n",
    "    print(f\"Running with batchsize: {batchsize}\")\n",
    "    fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "    preprocessor = BasePreprocessor()\n",
    "    streamers = fashionfactory.create_datastreamer(\n",
    "        batchsize=batchsize, preprocessor=preprocessor\n",
    "    )\n",
    "    train = streamers[\"train\"]\n",
    "    valid = streamers[\"valid\"]\n",
    "    trainstreamer = train.stream()\n",
    "    validstreamer = valid.stream()\n",
    "\n",
    "    for unit1 in units:\n",
    "        for unit2 in units:\n",
    "            if unit2 <= unit1:\n",
    "                run_subfolder = (\n",
    "                    f\"{subfolder}/batchsize_{batchsize}_units_{unit1}_{unit2}\"\n",
    "                )\n",
    "                settings = TrainerSettings(\n",
    "                    epochs=3,\n",
    "                    metrics=[accuracy],\n",
    "                    logdir=f\"{main_folder}/{run_subfolder}\",\n",
    "                    train_steps=len(train),\n",
    "                    valid_steps=len(valid),\n",
    "                    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    "                )\n",
    "                print(f\"Batchsize: {batchsize}, Units: {unit1}, {unit2}\")\n",
    "                model = NeuralNetwork(num_classes=10, units1=unit1, units2=unit2)\n",
    "\n",
    "                trainer = Trainer(\n",
    "                    model=model,\n",
    "                    settings=settings,\n",
    "                    loss_fn=loss_fn,\n",
    "                    optimizer=optim.Adam,\n",
    "                    traindataloader=trainstreamer,\n",
    "                    validdataloader=validstreamer,\n",
    "                    scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                )\n",
    "                trainer.loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script for changing depth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "preprocessor = BasePreprocessor()\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=64, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes: int, units1: int, units2: int, units3: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.units1 = units1\n",
    "        self.units2 = units2\n",
    "        self.units3 = units3\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units1, units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units2, units3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units3, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "units = [128, 64, 32, 16]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "main_folder = \"modellogs\"\n",
    "subfolder = \"change_depth\"\n",
    "\n",
    "for unit1 in units:\n",
    "    for unit2 in units:\n",
    "        if unit2 <= unit1:\n",
    "            for unit3 in units:\n",
    "                if unit3 <= unit2:\n",
    "                    run_subfolder = f\"{subfolder}/units_{unit1}_{unit2}_{unit3}\"\n",
    "                    settings = TrainerSettings(\n",
    "                        epochs=3,\n",
    "                        metrics=[accuracy],\n",
    "                        logdir=f\"{main_folder}/{run_subfolder}\",\n",
    "                        train_steps=len(train),\n",
    "                        valid_steps=len(valid),\n",
    "                        reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    "                    )\n",
    "                    print(f\"Units: {unit1}, {unit2}, {unit3}\")\n",
    "                    model = NeuralNetwork(\n",
    "                        num_classes=10, units1=unit1, units2=unit2, units3=unit3\n",
    "                    )\n",
    "\n",
    "                    trainer = Trainer(\n",
    "                        model=model,\n",
    "                        settings=settings,\n",
    "                        loss_fn=loss_fn,\n",
    "                        optimizer=optim.Adam,\n",
    "                        traindataloader=trainstreamer,\n",
    "                        validdataloader=validstreamer,\n",
    "                        scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                    )\n",
    "                    trainer.loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script for changing learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes: int, units1: int, units2: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.units1 = units1\n",
    "        self.units2 = units2\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units1, units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "units = [64, 32, 16]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "main_folder = \"modellogs\"\n",
    "subfolder = \"change_learningrate\"\n",
    "learningrates = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "\n",
    "for lr in learningrates:\n",
    "    for unit1 in units:\n",
    "        for unit2 in units:\n",
    "            if unit2 <= unit1:\n",
    "                run_subfolder = f\"{subfolder}/lr_{lr}_units_{unit1}_{unit2}\"\n",
    "                settings = TrainerSettings(\n",
    "                    epochs=3,\n",
    "                    metrics=[accuracy],\n",
    "                    logdir=f\"{main_folder}/{run_subfolder}\",\n",
    "                    train_steps=len(train),\n",
    "                    valid_steps=len(valid),\n",
    "                    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    "                    optimizer_kwargs={\"lr\": lr},\n",
    "                )\n",
    "                print(f\"Learning rate: {lr}, Units: {unit1}, {unit2}\")\n",
    "                model = NeuralNetwork(num_classes=10, units1=unit1, units2=unit2)\n",
    "\n",
    "                trainer = Trainer(\n",
    "                    model=model,\n",
    "                    settings=settings,\n",
    "                    loss_fn=loss_fn,\n",
    "                    optimizer=optim.Adam,\n",
    "                    traindataloader=trainstreamer,\n",
    "                    validdataloader=validstreamer,\n",
    "                    scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                )\n",
    "                trainer.loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script for changing optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "units = [64, 32, 16]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "main_folder = \"modellogs\"\n",
    "subfolder = \"change_optimizer\"\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizers = {\n",
    "    \"SGD\": optim.SGD,\n",
    "    \"Adam\": optim.Adam,\n",
    "    \"RMSprop\": optim.RMSprop,\n",
    "}\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "for opt_name, opt_class in optimizers.items():\n",
    "    for unit1 in units:\n",
    "        for unit2 in units:\n",
    "            if unit2 <= unit1:\n",
    "                run_subfolder = f\"{subfolder}/{opt_name}_units_{unit1}_{unit2}\"\n",
    "                settings = TrainerSettings(\n",
    "                    epochs=3,\n",
    "                    metrics=[accuracy],\n",
    "                    logdir=f\"{main_folder}/{run_subfolder}\",\n",
    "                    train_steps=len(train),\n",
    "                    valid_steps=len(valid),\n",
    "                    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    "                    optimizer_kwargs={\"lr\": learning_rate},\n",
    "                )\n",
    "                print(f\"Optimizer: {opt_name}, Units: {unit1}, {unit2}\")\n",
    "                model = NeuralNetwork(num_classes=10, units1=unit1, units2=unit2)\n",
    "\n",
    "                trainer = Trainer(\n",
    "                    model=model,\n",
    "                    settings=settings,\n",
    "                    loss_fn=loss_fn,\n",
    "                    optimizer=opt_class,  \n",
    "                    traindataloader=trainstreamer,\n",
    "                    validdataloader=validstreamer,\n",
    "                    scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                )\n",
    "                trainer.loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have set the ReportType to TOML, you will find in every log dir a model.toml and settings.toml file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment, and study the result with tensorboard. \n",
    "\n",
    "Locally, it is easy to do that with VS code itself. On the server, you have to take these steps:\n",
    "\n",
    "- in the terminal, `cd` to the location of the repository\n",
    "- activate the python environment for the shell. This can be done with `.venv\\Scripts\\activate`.\n",
    "- run `tensorboard --logdir=1-hypertuning-gridsearch/modellogs` in the terminal\n",
    "- tensorboard will launch at `localhost:6006` and vscode will notify you that the port is forwarded\n",
    "- you can either press the `launch` button in VScode or open your local browser at `localhost:6006`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio-example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
