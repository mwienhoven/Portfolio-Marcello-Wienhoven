{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from mltrainer import rnn_models, Trainer\n",
    "from torch import optim\n",
    "\n",
    "from mads_datasets import datatools\n",
    "import mltrainer\n",
    "mltrainer.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-04 23:11:42.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at C:\\Users\\mwien\\.cache\\mads_datasets\\gestures\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 2600/2600 [00:00<00:00, 3356.50it/s]\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 651/651 [00:00<00:00, 3228.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "from mltrainer.preprocessors import PaddedPreprocessor\n",
    "preprocessor = PaddedPreprocessor()\n",
    "\n",
    "gesturesdatasetfactory = DatasetFactoryProvider.create_factory(DatasetType.GESTURES)\n",
    "streamers = gesturesdatasetfactory.create_datastreamer(batchsize=32, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()\n",
    "x, y = next(iter(trainstreamer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mltrainer import TrainerSettings, ReportTypes\n",
    "from mltrainer.metrics import Accuracy\n",
    "\n",
    "accuracy = Accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    print(\"using cuda\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"using cpu\")\n",
    "\n",
    "# on my mac, at least for the BaseRNN model, mps does not speed up training\n",
    "# probably because the overhead of copying the data to the GPU is too high\n",
    "# so i override the device to cpu\n",
    "device = \"cpu\"\n",
    "# however, it might speed up training for larger models, with more parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the settings for the trainer and the different types of logging you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epochs: 16\n",
       "metrics: [Accuracy]\n",
       "logdir: gestures\n",
       "train_steps: 81\n",
       "valid_steps: 20\n",
       "reporttypes: [<ReportTypes.TOML: 'TOML'>, <ReportTypes.TENSORBOARD: 'TENSORBOARD'>, <ReportTypes.MLFLOW: 'MLFLOW'>]\n",
       "optimizer_kwargs: {'lr': 0.1, 'weight_decay': 1e-05}\n",
       "scheduler_kwargs: {'factor': 0.5, 'patience': 5}\n",
       "earlystop_kwargs: {'save': True, 'verbose': True, 'patience': 5, 'delta': 0.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = TrainerSettings(\n",
    "    epochs=16, # increase this to about 100 for training\n",
    "    metrics=[accuracy],\n",
    "    logdir=Path(\"gestures\"),\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TOML, ReportTypes.TENSORBOARD, ReportTypes.MLFLOW],\n",
    "    optimizer_kwargs={\"lr\": 0.1, \"weight_decay\": 1e-5},\n",
    "    scheduler_kwargs={\"factor\": 0.5, \"patience\": 5},\n",
    "    earlystop_kwargs = {\n",
    "        \"save\": True, # save every best model, and restore the best one\n",
    "        \"verbose\": True,\n",
    "        \"patience\": 5, # number of epochs with no improvement after which training will be stopped\n",
    "        \"delta\": 0.0, # minimum change to be considered an improvement\n",
    "    }\n",
    ")\n",
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    input_size: int\n",
    "    hidden_size: int\n",
    "    num_layers: int\n",
    "    output_size: int\n",
    "    dropout: float = 0.0\n",
    "\n",
    "class GRUmodel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config.input_size,\n",
    "            hidden_size=config.hidden_size,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True,\n",
    "            num_layers=config.num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(config.hidden_size, config.output_size)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x, _ = self.rnn(x)\n",
    "        last_step = x[:, -1, :]\n",
    "        yhat = self.linear(last_step)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/04 23:11:44 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/10/04 23:11:44 INFO mlflow.store.db.utils: Updating database tables\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Master Applied Data Science\\Year 2\\Semester 3 (Deep Learning & Model Deployment)\\Portfolio-Marcello-Wienhoven\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n",
      "\u001b[32m2025-10-04 23:11:44.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to gestures\\20251004-231144\u001b[0m\n",
      "\u001b[32m2025-10-04 23:11:45.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 56.94it/s]\n",
      "\u001b[32m2025-10-04 23:11:47.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 2.3265 test 1.4961 metric ['0.5406']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:11:47.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36msave_checkpoint\u001b[0m:\u001b[36m268\u001b[0m - \u001b[1mValidation loss (1.4961 --> 1.4961).Saving gestures\\20251004-231144\\checkpoint.pt ...\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 56.46it/s]\n",
      "\u001b[32m2025-10-04 23:11:49.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 1.5430 test 1.6564 metric ['0.5906']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:11:49.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.4961, current loss 1.6564.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 44.45it/s]\n",
      "\u001b[32m2025-10-04 23:11:51.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 2.3976 test 3.0069 metric ['0.4656']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:11:51.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.4961, current loss 3.0069.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:02<00:00, 35.14it/s]\n",
      "\u001b[32m2025-10-04 23:11:53.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 2.4758 test 2.0845 metric ['0.5844']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:11:53.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.4961, current loss 2.0845.Counter 3/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:02<00:00, 35.37it/s]\n",
      "\u001b[32m2025-10-04 23:11:56.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 2.6316 test 3.6422 metric ['0.4219']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:11:56.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.4961, current loss 3.6422.Counter 4/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:02<00:00, 31.67it/s]\n",
      "\u001b[32m2025-10-04 23:11:58.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 3.3374 test 3.8139 metric ['0.4078']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:11:58.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.4961, current loss 3.8139.Counter 5/5.\u001b[0m\n",
      "\u001b[32m2025-10-04 23:11:58.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mloop\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mInterrupting loop due to early stopping patience.\u001b[0m\n",
      "\u001b[32m2025-10-04 23:11:58.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mget_best\u001b[0m:\u001b[36m277\u001b[0m - \u001b[1mretrieving best model from gestures\\20251004-231144\\checkpoint.pt\u001b[0m\n",
      " 31%|\u001b[38;2;30;71;6m███▏      \u001b[0m| 5/16 [00:12<00:28,  2.57s/it]\n",
      "c:\\Master Applied Data Science\\Year 2\\Semester 3 (Deep Learning & Model Deployment)\\Portfolio-Marcello-Wienhoven\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n",
      "\u001b[32m2025-10-04 23:11:59.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to gestures\\20251004-231159\u001b[0m\n",
      "\u001b[32m2025-10-04 23:11:59.013\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 59.89it/s]\n",
      "\u001b[32m2025-10-04 23:12:00.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 2.4901 test 1.2902 metric ['0.5922']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:00.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36msave_checkpoint\u001b[0m:\u001b[36m268\u001b[0m - \u001b[1mValidation loss (1.2902 --> 1.2902).Saving gestures\\20251004-231159\\checkpoint.pt ...\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 54.21it/s]\n",
      "\u001b[32m2025-10-04 23:12:02.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 1.4552 test 1.2680 metric ['0.6312']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:02.178\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36msave_checkpoint\u001b[0m:\u001b[36m268\u001b[0m - \u001b[1mValidation loss (1.2902 --> 1.2680).Saving gestures\\20251004-231159\\checkpoint.pt ...\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 58.00it/s]\n",
      "\u001b[32m2025-10-04 23:12:03.735\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 1.6646 test 1.5020 metric ['0.6484']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:03.736\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.2680, current loss 1.5020.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 46.45it/s]\n",
      "\u001b[32m2025-10-04 23:12:05.661\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 1.7534 test 2.7439 metric ['0.4641']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:05.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.2680, current loss 2.7439.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 41.54it/s]\n",
      "\u001b[32m2025-10-04 23:12:07.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 2.3430 test 2.9651 metric ['0.4297']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:07.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.2680, current loss 2.9651.Counter 3/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:02<00:00, 36.58it/s]\n",
      "\u001b[32m2025-10-04 23:12:10.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 5.4480 test 6.0059 metric ['0.3047']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:10.173\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.2680, current loss 6.0059.Counter 4/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:03<00:00, 25.75it/s]\n",
      "\u001b[32m2025-10-04 23:12:13.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 4.6390 test 3.2877 metric ['0.4578']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:13.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.2680, current loss 3.2877.Counter 5/5.\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:13.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mloop\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mInterrupting loop due to early stopping patience.\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:13.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mget_best\u001b[0m:\u001b[36m277\u001b[0m - \u001b[1mretrieving best model from gestures\\20251004-231159\\checkpoint.pt\u001b[0m\n",
      " 38%|\u001b[38;2;30;71;6m███▊      \u001b[0m| 6/16 [00:14<00:24,  2.42s/it]\n",
      "\u001b[32m2025-10-04 23:12:13.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLogging to gestures\\20251004-231213\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:13.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 56.83it/s]\n",
      "\u001b[32m2025-10-04 23:12:15.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 0 train 3.4875 test 2.2669 metric ['0.5078']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:15.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36msave_checkpoint\u001b[0m:\u001b[36m268\u001b[0m - \u001b[1mValidation loss (2.2669 --> 2.2669).Saving gestures\\20251004-231213\\checkpoint.pt ...\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 55.59it/s]\n",
      "\u001b[32m2025-10-04 23:12:16.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 1 train 1.7197 test 1.4799 metric ['0.5062']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:16.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36msave_checkpoint\u001b[0m:\u001b[36m268\u001b[0m - \u001b[1mValidation loss (2.2669 --> 1.4799).Saving gestures\\20251004-231213\\checkpoint.pt ...\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 50.90it/s]\n",
      "\u001b[32m2025-10-04 23:12:18.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 2 train 1.6236 test 1.2815 metric ['0.6312']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:18.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36msave_checkpoint\u001b[0m:\u001b[36m268\u001b[0m - \u001b[1mValidation loss (1.4799 --> 1.2815).Saving gestures\\20251004-231213\\checkpoint.pt ...\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 46.84it/s]\n",
      "\u001b[32m2025-10-04 23:12:20.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 3 train 1.9565 test 2.1013 metric ['0.4875']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:20.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.2815, current loss 2.1013.Counter 1/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:01<00:00, 40.67it/s]\n",
      "\u001b[32m2025-10-04 23:12:22.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 4 train 2.4822 test 2.3660 metric ['0.5469']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:22.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.2815, current loss 2.3660.Counter 2/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:02<00:00, 31.60it/s]\n",
      "\u001b[32m2025-10-04 23:12:25.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 5 train 2.3762 test 2.8755 metric ['0.5047']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:25.339\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.2815, current loss 2.8755.Counter 3/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:02<00:00, 30.15it/s]\n",
      "\u001b[32m2025-10-04 23:12:28.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 6 train 2.8408 test 3.5640 metric ['0.4703']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:28.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.2815, current loss 3.5640.Counter 4/5.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 81/81 [00:02<00:00, 30.42it/s]\n",
      "\u001b[32m2025-10-04 23:12:31.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m209\u001b[0m - \u001b[1mEpoch 7 train 2.4533 test 2.0510 metric ['0.5703']\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:31.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mbest loss: 1.2815, current loss 2.0510.Counter 5/5.\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:31.044\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mloop\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mInterrupting loop due to early stopping patience.\u001b[0m\n",
      "\u001b[32m2025-10-04 23:12:31.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mget_best\u001b[0m:\u001b[36m277\u001b[0m - \u001b[1mretrieving best model from gestures\\20251004-231213\\checkpoint.pt\u001b[0m\n",
      " 44%|\u001b[38;2;30;71;6m████▍     \u001b[0m| 7/16 [00:17<00:22,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"gestures\")\n",
    "modeldir = Path(\"gestures\").resolve()\n",
    "if not modeldir.exists():\n",
    "    modeldir.mkdir(parents=True)\n",
    "    \n",
    "n_repeats = 3\n",
    "\n",
    "for repeat in range(n_repeats):\n",
    "    print(f\"Repeat {repeat+1}/{n_repeats}\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # Set MLflow tags to record metadata about the model and developer\n",
    "        mlflow.set_tag(\"model\", f\"{repeat}_GRU_16epochs_0.5drop_128hidsize_lr-0.1\")\n",
    "        mlflow.set_tag(\"dev\", \"Marcello\")\n",
    "        # Log hyperparameters to MLflow\n",
    "\n",
    "        mlflow.log_param(\"epochs\", settings.epochs)\n",
    "\n",
    "        mlflow.log_param(\"learning_rate\", settings.optimizer_kwargs.get(\"lr\", None))\n",
    "        \n",
    "        config = ModelConfig(\n",
    "            input_size=3,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,\n",
    "            output_size=20,\n",
    "            dropout=0.5,\n",
    "        )\n",
    "\n",
    "        model = GRUmodel(\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            settings=settings,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optim.Adam,\n",
    "            traindataloader=trainstreamer,\n",
    "            validdataloader=validstreamer,\n",
    "            scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            device=device,\n",
    "        )\n",
    "        trainer.loop()\n",
    "\n",
    "        if not settings.earlystop_kwargs[\"save\"]:\n",
    "            tag = datetime.now().strftime(\"%Y%m%d-%H%M-\")\n",
    "            modelpath = modeldir / (tag + \"model.pt\")\n",
    "            torch.save(model, modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open MLFlow\n",
    "\"\"\"\n",
    "```bash\n",
    "mlflow server \\\n",
    "    --backend-store-uri sqlite:///mlflow.db \\\n",
    "    --host 127.0.0.1 \\ \n",
    "    --port 5000 \\\n",
    "        \n",
    "mlflow server --backend-store-uri sqlite:///3-hypertuning-rnn/mlflow.db --host 127.0.0.1 --port 5000\n",
    "```\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio-example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
